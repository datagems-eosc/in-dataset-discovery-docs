{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Overview","text":"<p>Welcome to the documentation for the In-Dataset Discovery Service.</p> <p>This service provides an API for performing natural language exploration queries on structured or unstructured data within a dataset. It is designed to help users quickly find insights by leveraging advanced algorithms and LLM capabilities, including geospatial queries and text-to-SQL conversion.</p>"},{"location":"#how-to-use-this-documentation","title":"How to Use This Documentation","text":"<ul> <li>System: Learn about the high-level Architecture, Security model, and Datastores.</li> <li>API: Find detailed information on the API endpoints and error codes.</li> <li>Setup &amp; Monitoring: Get instructions on Deployment, Configuration, and Logging.</li> </ul>"},{"location":"api-overview/","title":"API Overview","text":"<p>The In-Dataset Discovery service exposes a RESTful API for performing in-dataset exploration operations, including geospatial queries and text-to-SQL conversion.</p>"},{"location":"api-overview/#base-url","title":"Base URL","text":"<p>The API is served from the root of the application. All endpoint paths are relative to the base URL where the service is deployed:</p> <p>Base URL: https://datagems-dev.scayle.es/in-dataset-discovery/</p>"},{"location":"api-overview/#authentication","title":"Authentication","text":"<p>The <code>/text2geo</code> and <code>/text2sql</code> endpoints can accessed only with authentication. See the Security page for more details. The <code>/health</code> endpoint is publicly accessible.</p>"},{"location":"api-overview/#endpoints","title":"Endpoints","text":""},{"location":"api-overview/#text-to-geospatial-objects-query","title":"Text to Geospatial Objects Query","text":"<p>Performs geospatial queries using natural language to find locations, generate OverpassQL queries and return geospatial objects.</p> <ul> <li>Endpoint: <code>GET /text2geo</code></li> <li>Description: Processes a natural language geospatial question, identifies the place using Wikidata, generates an OverpassQL query, and returns geospatial objects, resulting in coordinates, GeoJSON data, and bounding boxes.</li> <li> <p>Query Parameters:</p> <ul> <li><code>question</code> (required): The natural language geospatial query question.</li> </ul> </li> <li> <p>Example Request: <pre><code>GET /text2geo?question=What are the coordinates in Zurich?\n</code></pre></p> </li> <li> <p>Response Body (Success): <pre><code>{\n  \"most_relevant_wikidata\": {\n    \"place\": \"Zurich\",\n    \"reasoning\": \"The question asks for the city of Zurich, which is best represented by the entity with ID Q72. This entity is described as the capital of the canton of Zurich and is a municipality and city in Switzerland. It also has 'found_osm_json: True', which gives it priority if multiple similar entities were found. Other entities like universities or sports clubs are not relevant to the context of the question, which specifically asks for the city.\",\n    \"wiki_id\": \"Q72\",\n    \"wiki_properties\": {\n      \"aliases\": \"City of Zurich, ZH, Stadt Z\u00fcrich, Zurich, Switzerland, Z\u00fcrich\",\n      \"coordinate location\": \"47.37444444444444, 8.54111111111111\",\n      \"country\": \"Switzerland, Old Swiss Confederacy, Helvetic Republic, Switzerland\",\n      \"description\": \"capital of the canton of Zurich, Switzerland\",\n      \"found_osm_json\": true,\n      \"instance of\": \"municipality of Switzerland, city of Switzerland, cantonal capital of Switzerland, college town, largest city, big city\",\n      \"label\": \"Zurich\",\n      \"located in the administrative territorial entity\": \"Z\u00fcrich District\",\n      \"part of\": \"Greater Zurich Area, Zurich metropolitan area, Canton of Z\u00fcrich\"\n    }\n  },\n  \"oql\": {\n    \"OQL\": \"area[\\\"ISO3166-1\\\"=\\\"CH\\\"]-&gt;.searchArea;(nwr[\\\"wikidata\\\"=\\\"Q72\\\"];);\",\n    \"reasoning\": \"Generated Overpass QL query using Wikidata ID Q72 and country ISO alpha-2 code CH.\"\n  },\n  \"place\": \"zurich\",\n  \"results\": {\n    \"bounds\": {\n      \"maxlat\": 47.4346662,\n      \"maxlon\": 8.6254413,\n      \"minlat\": 47.3202187,\n      \"minlon\": 8.4480061\n    },\n    \"center\": [\n      47.4,\n      8.5\n    ],\n    \"geojson_data\": {\n      \"features\": [],\n      \"type\": \"FeatureCollection\"\n    },\n    \"points\": [\n      {\n        \"lat\": \"47.4\",\n        \"lon\": \"8.5\"\n      }\n    ]\n  }\n}\n</code></pre></p> </li> </ul>"},{"location":"api-overview/#text-to-sql-query","title":"Text-to-SQL Query","text":"<p>Converts natural language questions into SQL queries and executes them against a specified database.</p> <ul> <li>Endpoint: <code>POST /text2sql</code></li> <li> <p>Description: Takes a natural language question and database connection information, generates a SQL query using LLM capabilities, and executes it to return results.     The parameters are:</p> <ul> <li><code>db_info</code>: The database connection information.</li> <li><code>results</code>: The results containing points for SQL query. The <code>results</code> parameter is optional. If not provided, the SQL query will be generated without parameters.</li> <li>The <code>db_info</code> parameter is required. The <code>db_info</code> parameter is a dictionary with the following keys:</li> <li><code>db_host</code>: The database host.</li> <li><code>db_username</code>: The database username.</li> <li><code>db_pass</code>: The database password.</li> <li><code>db_database</code>: The database name.</li> <li><code>db_schema</code>: The database schema.</li> <li><code>db_port</code>: The database port.</li> </ul> </li> <li> <p>Request Body with parameters:</p> <pre><code>{\n  \"question\": \"What are the average mean temperatures in the coordinates (lon, lat) in year 2020?\",\n  \"parameters\": {\n    \"results\": {\n      \"points\": [[8.4, 47.4], [8.5, 47.4]]\n    },\n    \"db_info\": {\n      \"db_host\": \"the_db_host_ip\",\n      \"db_username\": \"db_username\",\n      \"db_pass\": \"db_password\",\n      \"db_database\": \"database_name\",\n      \"db_schema\": \"database_schema\",\n      \"db_port\": 5432\n    }\n  }\n}\n</code></pre> </li> <li> <p>Response Body with parameters (Success):</p> <pre><code>{\n  \"status\": \"success\",\n  \"message\": \"Successfully generated SQL query on the first attempt.\",\n  \"question\": \"What are the average mean temperatures...\",\n  \"model_name\": \"gpt-4o\",\n  \"sql_pattern\": \"SELECT AVG(tmean - 273.15)...\",\n  \"sql_query\": \"SELECT AVG(tmean - 273.15) AS average_mean_temperature_celsius...\",\n  \"sql_results\": {\n    \"status\": \"success\",\n    \"data\": [{\"average_mean_temperature_celsius\": 10.5}]\n  }\n}\n</code></pre> </li> <li> <p>Request Body without parameters:</p> <pre><code>{\n  \"question\": \"What are the average mean temperatures at lat 47.4, lon 8.4 in year 2020?\",\n  \"parameters\": {\n    \"db_info\": {\n      \"db_host\": \"the_db_host_ip\",\n      \"db_username\": \"db_username\",\n      \"db_pass\": \"db_password\",\n      \"db_database\": \"database_name\",\n      \"db_schema\": \"database_schema\",\n      \"db_port\": 5432\n    }\n  }\n}\n</code></pre> </li> <li> <p>Response Body without parameters (Success):</p> <pre><code>{\n\"status\": \"success\",\n\"message\": \"Successfully generated SQL query on the first attempt.\",\n\"params\": null,\n\"question\": \"Show the minimum temperature at lat 47.4, lon 8.5 in 2005\",\n\"model_name\": \"gpt-4o\",\n\"sql_pattern\": null,\n\"input_params\": null,\n\"output_params\": null,\n\"reasoning\": \"The query is designed to find the minimum temperature recorded in the year 2005 at the specified latitude and longitude coordinates (47.4, 8.5). The temperatures in the 'meteo_tmin' table are stored in Kelvin, so they are converted to Celsius by subtracting 273.15. We use the WHERE clause to filter records that match the given coordinates and year, and then SELECT the MIN value of these converted temperatures.\",\n\"sql_query\": \"SELECT MIN(tmin - 273.15) AS min_temp_celsius FROM era5_land2.meteo_tmin WHERE round(latitude::numeric, 1) = 47.4 AND round(longitude::numeric,1) = 8.5 AND EXTRACT(YEAR FROM time) = 2005;\",\n\"sql_results\": {\n  \"status\": \"success\",\n  \"data\": [\n    {\n      \"min_temp_celsius\": -12.898565673828102\n    }\n    ]\n  }\n}\n</code></pre> </li> </ul>"},{"location":"api-overview/#health-check","title":"Health Check","text":"<p>Verifies the operational status of the API.</p> <ul> <li>Endpoint: <code>GET /health</code></li> <li>Description: Checks the availability of the API service. Returns a <code>200 OK</code> if the service is healthy. This endpoint does not require authentication.</li> <li> <p>Response Body (Success):</p> <pre><code>{\n  \"status\": \"ok\"\n}\n</code></pre> </li> </ul>"},{"location":"architecture/","title":"System Architecture","text":"<p>The In-Dataset Discovery Service is a self-contained application designed to act as a specialized query layer within the DataGEMS ecosystem.</p>"},{"location":"architecture/#core-components","title":"Core Components","text":"<ol> <li> <p>FastAPI Application: The heart of the service is a Python web application built with FastAPI. It exposes the REST API, handles incoming HTTP requests, and orchestrates all internal operations.</p> </li> <li> <p>Geospatial Component: A module that processes natural language geospatial queries, identifies places using Wikidata, generates OverpassQL queries, and retrieves geospatial data from OpenStreetMap via the Overpass API.</p> </li> <li> <p>Text-to-SQL Component: An intelligent module that converts natural language questions into SQL queries using LLM capabilities. It handles database schema understanding, query generation, and execution.</p> </li> <li> <p>Security Layer: This layer intercepts all incoming requests to perform authentication and authorization. It integrates with an external OIDC provider to validate JWTs and with the DataGEMS Gateway to fetch user-specific permissions.</p> </li> </ol>"},{"location":"architecture/#request-flow","title":"Request Flow","text":""},{"location":"architecture/#geospatial-query-flow","title":"Geospatial Query Flow","text":"<ol> <li>A user sends a <code>GET /geospatial</code> request with a natural language question about a location.</li> <li>The service uses Wikidata to identify the most relevant place entity.</li> <li>An OverpassQL query is generated based on the Wikidata information.</li> <li>The Overpass API is queried to retrieve geospatial data.</li> <li>Results are formatted and returned, including coordinates, GeoJSON data, and bounding boxes.</li> </ol>"},{"location":"architecture/#text-to-sql-query-flow","title":"Text-to-SQL Query Flow","text":"<ol> <li>A user sends a <code>POST /text2sql</code> request with a natural language question and database connection information.</li> <li>The Security Layer validates the token and checks for the required user roles (if authentication is enabled).</li> <li>The Text-to-SQL Component uses an LLM to analyze the question and database schema.</li> <li>A SQL query pattern is generated and parameterized with the provided coordinates or other parameters.</li> <li>The SQL query is executed against the specified database.</li> <li>Results are returned with the generated SQL and execution results.</li> </ol>"},{"location":"automations/","title":"Automations","text":"<p>This project leverages GitHub Actions to automate builds, code analysis, security scanning, and documentation deployment. The following sections detail the specific workflows configured for this repository.</p>"},{"location":"automations/#dockerfile","title":"Dockerfile","text":"<p>The creation of a production-ready container image is fully automated by the <code>Dockerfile</code> in the root of the repository. This build process ensures a consistent environment for the application.</p> <ul> <li> <p>Stage 1 (Builder): This stage prepares the environment by installing all necessary build-time dependencies, including the full set of Python packages.</p> </li> <li> <p>Stage 2 (Final): This stage constructs the final, lean image by copying only the essential artifacts from the builder stage. It creates a non-root <code>appuser</code> for security, copies the application code, and sets the <code>unicorn</code> server as the entry point.</p> </li> </ul>"},{"location":"automations/#docker-image-publishing","title":"Docker image publishing","text":"<p>Workflow: <code>.github/workflows/docker-publish.yml</code></p> <p>This workflow automates the process of building and publishing the service's Docker image to the GitHub Container Registry (ghcr.io).</p> <ul> <li>Trigger: This workflow runs automatically whenever a new Git tag matching the pattern <code>v*</code> (e.g., <code>v1.0.0</code>, <code>v1.1.0</code>) is pushed to the repository.</li> <li>Process:<ol> <li>Logs into the GitHub Container Registry using a temporary <code>GITHUB_TOKEN</code>.</li> <li>Uses the <code>docker/metadata-action</code> to automatically generate appropriate Docker tags based on the Git tag.</li> <li>Builds the Docker image using the <code>Dockerfile</code>.</li> <li>Pushes the newly built and tagged image to the <code>ghcr.io/datagems-eosc/in-data-exploration</code> repository, making it available for deployment.</li> </ol> </li> </ul>"},{"location":"automations/#vulnerability-scanning","title":"Vulnerability Scanning","text":"<p>Workflow: <code>.github/workflows/vulnerability-scan-on-demand.yml</code></p> <p>This workflow provides on-demand security scanning of the project's configuration and Docker images using the Trivy security scanner.</p> <ul> <li>Trigger: This workflow is run manually from the GitHub Actions UI (<code>workflow_dispatch</code>). It requires an <code>image_tag</code> as input to specify which published Docker image to scan.</li> <li>Process:<ol> <li>Configuration Scan: Scans the <code>Dockerfile</code> and other repository configuration files for potential security misconfigurations.</li> <li>Image Scan: Pulls the specified Docker image from the GitHub Container Registry and scans its operating system packages and Python libraries for known vulnerabilities (CVEs) with <code>CRITICAL</code> or <code>HIGH</code> severity.</li> <li>Artifacts: The JSON reports from both the configuration scan and the image scan are uploaded as build artifacts for review.</li> </ol> </li> </ul>"},{"location":"automations/#static-code-analysis","title":"Static Code Analysis","text":"<p>Workflow: <code>.github/workflows/code-scan-on-demand.yml</code></p> <p>This workflow performs in-depth static code analysis using GitHub CodeQL to find potential bugs, security vulnerabilities, and quality issues in the codebase.</p> <ul> <li>Trigger: This workflow is run manually from the GitHub Actions UI (<code>workflow_dispatch</code>).</li> <li>Process:<ol> <li>It runs two separate analysis jobs: one for the Python source code and one for the GitHub Actions workflow files themselves.</li> <li>For each language, it initializes CodeQL using an extended set of security and quality queries.</li> <li>It performs the analysis and uploads the results directly to the repository's \"Security\" tab under \"Code scanning alerts\".</li> </ol> </li> </ul>"},{"location":"automations/#code-metrics","title":"Code Metrics","text":"<p>Workflow: <code>.github/workflows/code-metrics-on-demand.yml</code></p> <p>This workflow generates a report on the complexity and maintainability of the application's Python code using the Radon library.</p> <ul> <li>Trigger: This workflow is run manually from the GitHub Actions UI (<code>workflow_dispatch</code>).</li> <li>Process:<ol> <li>Installs the <code>radon</code> Python package.</li> <li>Runs three types of analysis on the <code>src/app</code> directory:<ul> <li>Maintainability Index (MI): A score from 0-100 indicating how easy the code is to maintain.</li> <li>Cyclomatic Complexity (CC): Measures the number of independent paths through the code, indicating its complexity.</li> <li>Raw Lines of Code (LOC): Provides basic code size metrics.</li> </ul> </li> <li>The complete report is compiled into a text file and uploaded as a build artifact named <code>code-metrics-report</code>.</li> </ol> </li> </ul>"},{"location":"automations/#documentation","title":"Documentation","text":"<p>Workflow: <code>.github/workflows/deploy-docs-on-demand.yml</code></p> <p>The deployment of this documentation site is automated by a dedicated GitHub Action workflow.</p> <ul> <li>Trigger: This workflow is run manually from the GitHub Actions UI (<code>workflow_dispatch</code>).</li> <li>Process: When triggered with a version number (e.g., <code>1.0.0</code>), the workflow:<ol> <li>Installs <code>mkdocs</code>, <code>mike</code>, and other required tools.</li> <li>Builds the static HTML site from the markdown source files in the <code>docs/</code> directory.</li> <li>Uses the <code>mike</code> tool to commit the built site to the <code>gh-pages</code> branch, organized under the specified version.</li> <li>Updates the <code>latest</code> alias to point to this newly deployed version, ensuring visitors see the most recent documentation by default.</li> </ol> </li> </ul>"},{"location":"configuration/","title":"Configuration","text":"<p>The In-Dataset Discovery service is configured using a combination of a local configuration file and environment variables.</p>"},{"location":"configuration/#configuration-files","title":"Configuration files","text":"<p>For local development, the service can be configured using a <code>.env</code> file placed in the root of the repository. The application will automatically load variables from this file on startup.</p> <p>Warning The <code>.env</code> file is intended for local development only and should be added to your <code>.gitignore</code> file to prevent committing secrets to version control.</p>"},{"location":"configuration/#example-env-file","title":"Example <code>.env</code> file","text":"<pre><code># .env\n\n# OIDC Authentication\nOIDC_ISSUER_URL=\"https://datagems-dev.scayle.es/oauth/realms/dev\"\nOIDC_AUDIENCE=\"in-data-exploration\"\nGATEWAY_API_URL=\"https://datagems-dev.scayle.es/gw\"\n\n# OpenAI Configuration\nOPENAI_API_KEY=\"sk-your-openai-api-key\"\nOPENAI_MODEL_NAME=\"gpt-4o\"\n\n# LangChain Configuration (optional, for LangSmith)\nLANGCHAIN_API_KEY=\"ls-your-langchain-api-key\"\n\n# Overpass API Configuration\nOVERPASS_API_URL=\"https://overpass-api.de/api/\"\n\n# Secrets (for local use only)\nIdpClientSecret=\"your-local-dev-secret\"\n</code></pre>"},{"location":"configuration/#environment-overrides","title":"Environment Overrides","text":"<p>In any environment, especially in production, environment variables will always override values set in the <code>.env</code> file. This is the standard and recommended way to configure the service when deploying it as a container.</p> <p>The following table lists all available configuration variables:</p> Variable Description Example OIDC_ISSUER_URL The base URL of the OIDC identity provider for token validation. https://datagems-dev.scayle.es/oauth/realms/dev OIDC_AUDIENCE The audience claim that must be present in the JWT. in-data-exploration GATEWAY_API_URL The base URL of the DataGEMS Gateway API, used to fetch user permissions. https://datagems-dev.scayle.es/gw IdpClientSecret The client secret for the identity provider. your-secret OPENAI_API_KEY The API key for accessing OpenAI services (required for LLM features). sk-... LANGCHAIN_API_KEY The API key for accessing LangChain services, e.g., LangSmith (optional). ls-... OPENAI_MODEL_NAME The name of the OpenAI model to use. gpt-4o OVERPASS_API_URL The URL for the Overpass API. https://overpass-api.de/api/"},{"location":"configuration/#secrets","title":"Secrets","text":"<p>Certain configuration variables contain sensitive information and must be handled securely.</p>"},{"location":"configuration/#identified-secrets","title":"Identified Secrets","text":"<ul> <li>OPENAI_API_KEY: Contains the API key for OpenAI services.</li> <li>LANGCHAIN_API_KEY: Contains the API key for LangChain services (if used).</li> <li>IdpClientSecret: The client secret used for communication with the identity provider.</li> </ul> <p>Note: Database credentials for the <code>/text2sql</code> endpoint are provided by users in their requests and are not stored by the service.</p>"},{"location":"datastore/","title":"Datastores","text":"<p>The In-Dataset Discovery service interacts with various data sources and external services to provide its functionality.</p>"},{"location":"datastore/#external-data-sources","title":"External Data Sources","text":""},{"location":"datastore/#overpass-api","title":"Overpass API","text":"<p>The service uses the Overpass API to retrieve geospatial data from OpenStreetMap.</p> <ul> <li>Technology: Overpass API (OpenStreetMap)</li> <li>Purpose: Provides geospatial data retrieval for the <code>/geospatial</code> endpoint.</li> <li>Configuration: The Overpass API URL can be configured via the <code>OVERPASS_API_URL</code> environment variable (default: <code>https://overpass-api.de/api/</code>).</li> </ul>"},{"location":"datastore/#user-provided-databases","title":"User-Provided Databases","text":"<p>For the <code>/text2sql</code> endpoint, the service connects to databases specified by users in their requests.</p> <ul> <li>Technology: PostgreSQL (and potentially other SQL databases)</li> <li>Purpose: Executes generated SQL queries to retrieve data from user-specified databases.</li> <li>Connection: Database connection information (host, port, username, password, database name, schema) is provided in each request. The service uses <code>psycopg2</code> for PostgreSQL connections.</li> </ul>"},{"location":"datastore/#wikidata","title":"Wikidata","text":"<p>The service uses Wikidata to identify and enrich place information for geospatial queries.</p> <ul> <li>Technology: Wikidata REST API</li> <li>Purpose: Identifies places mentioned in natural language queries and retrieves their properties (coordinates, country codes, etc.) for generating OverpassQL queries.</li> </ul>"},{"location":"deployment/","title":"Deployment","text":"<p>The In-Dataset Discovery service is designed for containerized deployment using Docker. This guide provides instructions for building the image, configuring the service, and understanding its dependencies.</p>"},{"location":"deployment/#docker","title":"Docker","text":"<p>The primary method for deploying the service is via a Docker container. The repository includes a <code>Dockerfile</code> for the build process.</p>"},{"location":"deployment/#dockerfile-stages","title":"Dockerfile Stages","text":"<ol> <li> <p>Builder Stage:</p> <ul> <li>Starts from a <code>python:3.11-slim</code> base image.</li> <li>Installs all Python dependencies from <code>pyproject.toml</code> using <code>uv</code>.</li> </ul> </li> <li> <p>Final Stage:</p> <ul> <li>Starts from a fresh <code>python:3.11-slim</code> image.</li> <li>Creates a non-root user (<code>appuser</code>) for security.</li> <li>Copies the installed Python packages from the builder stage.</li> <li>Copies the application source code into the container.</li> <li>Sets <code>appuser</code> as the active user.</li> <li>Exposes port <code>8080</code>.</li> <li>Includes a <code>HEALTHCHECK</code> instruction that queries the <code>/health</code> endpoint to monitor container health.</li> <li>The default command (<code>CMD</code>) starts the application using <code>uvicorn</code>, making it production-ready.</li> </ul> </li> </ol>"},{"location":"deployment/#build-and-run","title":"Build and Run","text":"<p>To build and run the container, use the standard Docker commands:</p> <pre><code># 1. Build the Docker image\ndocker build -t in-data-exploration .\n\n# 2. Run the container\n# Note: You must provide all required environment variables.\ndocker run -p 8080:8080 \\\n  --env-file .env \\\n  --name in-data-exploration-service \\\n  in-data-exploration\n</code></pre>"},{"location":"deployment/#configuration","title":"Configuration","text":"<p>The service is configured entirely through environment variables. This allows for flexible deployment across different environments without changing the container image.</p> Variable Description Example OIDC_ISSUER_URL The base URL of the OIDC identity provider for token validation. https://datagems-dev.scayle.es/oauth/realms/dev OIDC_AUDIENCE The audience claim that must be present in the JWT. in-data-exploration GATEWAY_API_URL The base URL of the DataGEMS Gateway API, used to fetch user permissions. https://datagems-dev.scayle.es/gw IdpClientSecret The client secret for the identity provider (taken as secret from Vault). your-secret OPENAI_API_KEY The API key for accessing OpenAI services (required for LLM features). sk-... LANGCHAIN_API_KEY The API key for accessing LangChain services, e.g., LangSmith. ls-... OPENAI_MODEL_NAME The name of the OpenAI model to use. gpt-4o OVERPASS_API_URL The URL for the Overpass API. https://overpass-api.de/api/"},{"location":"deployment/#dependencies","title":"Dependencies","text":"<p>The service requires several external systems and resources to be available at runtime to function correctly.</p>"},{"location":"deployment/#runtime-dependencies","title":"Runtime Dependencies","text":"<p>OpenAI API: - Description: Required for LLM-powered features including text-to-SQL conversion. - Requirement: A valid <code>OPENAI_API_KEY</code> must be configured.</p> <p>Overpass API: - Description: Used for retrieving geospatial data from OpenStreetMap for the <code>/geospatial</code> endpoint. - Requirement: Network access to the Overpass API (default: <code>https://overpass-api.de/api/</code>). Can be configured via <code>OVERPASS_API_URL</code>.</p> <p>OIDC Provider: - Description: An OpenID Connect compliant identity provider (e.g., Keycloak) is necessary for authenticating users by validating their JWTs (for protected endpoints). - Requirement: The <code>OIDC_ISSUER_URL</code> and <code>OIDC_AUDIENCE</code> must be correctly configured to point to the identity provider.</p> <p>DataGEMS Gateway (Optional): - Description: The service may communicate with the DataGEMS Gateway API to fetch dataset-level permissions for users. - Requirement: The <code>GATEWAY_API_URL</code> must be configured if gateway integration is needed.</p> <p>User Databases: - Description: For the <code>/text2sql</code> endpoint, users provide database connection information in their requests. The service connects to these databases to execute queries. - Requirement: Network access to user-specified databases. The service uses <code>psycopg2</code> for PostgreSQL connections.</p>"},{"location":"error-codes/","title":"Status &amp; Error Codes","text":"<p>The API uses standard HTTP status codes to indicate the success or failure of a request.</p> Status Code Meaning Description <code>200 OK</code> Success The request was successful. <code>400 Bad Request</code> Validation Error The request body is malformed or contains invalid data. The response body will contain details about the validation error. <code>401 Unauthorized</code> Authentication Error The request lacks a valid JWT, the token is expired, or its signature is invalid. <code>403 Forbidden</code> Authorization Error The user is authenticated but does not have the required role (<code>user</code> or <code>dg_user</code>) to perform the action. <code>422 Unprocessable Entity</code> Validation Error The request body is well-formed but contains semantic errors (e.g., missing required fields, invalid parameter values). The response body will contain details about the validation error. <code>424 Failed Dependency</code> Downstream Service Error The API could not communicate with a required dependency, such as the OIDC provider, OpenAI API, Overpass API, or a user-specified database. The response body will contain details about the source of the failure. <code>500 Internal Server Error</code> Unexpected Error An unexpected error occurred on the server while processing the request. This may occur due to agent failures, LLM errors, or other internal issues. <code>503 Service Unavailable</code> Service Not Ready A core component failed to initialize at startup. This is typically seen during health checks if the service is not fully ready."},{"location":"faq/","title":"FAQ & Known Issues","text":"<p>TODO</p>"},{"location":"license/","title":"License","text":""},{"location":"logging/","title":"Logging &amp; Accounting","text":"<p>The service uses <code>structlog</code> for structured, JSON-formatted logging.</p>"},{"location":"logging/#log-structure","title":"Log Structure","text":"<p>All log entries are formatted as JSON objects with a consistent structure, including:</p> <ul> <li><code>@t</code>: ISO 8601 timestamp.</li> <li><code>@mt</code>: The log message (event).</li> <li><code>@l</code>: The log level (e.g., \"Info\", \"Warning\", \"Error\").</li> <li><code>DGCorrelationId</code>: A unique ID that ties together all log entries for a single HTTP request.</li> <li><code>SourceContext</code>: The name of the logger that produced the message.</li> <li><code>Application</code>: The name of the application (<code>in-data-exploration-api</code>).</li> </ul> <p>Additional key-value pairs are added to provide context for specific events.</p>"},{"location":"logging/#correlation-id","title":"Correlation ID","text":"<p>Every HTTP request is assigned a correlation ID. - If the incoming request includes an <code>x-tracking-correlation</code> header, its value is used. - Otherwise, a new UUID is generated.</p> <p>This ID is included in every log message generated during the processing of that request and is also returned in the <code>x-tracking-correlation</code> response header. This allows for easy tracing of a request's entire lifecycle through the system and across different services.</p>"},{"location":"logging/#request-logging","title":"Request Logging","text":"<p>A middleware automatically logs the end of every HTTP request (except for health checks), capturing:</p> <ul> <li><code>RequestMethod</code> (e.g., GET, POST)</li> <li><code>RequestPath</code></li> <li><code>StatusCode</code></li> <li><code>ProcessTimeMS</code></li> <li><code>ResponseSize</code></li> </ul>"},{"location":"maintenance/","title":"Maintenance","text":"<p>This document outlines key maintenance tasks for the In-Dataset Discovery Service.</p>"},{"location":"maintenance/#regular-maintenance-tasks","title":"Regular Maintenance Tasks","text":""},{"location":"maintenance/#monitoring-service-health","title":"Monitoring Service Health","text":"<p>Regularly monitor the service health using the <code>/health</code> endpoint to ensure all components are functioning correctly.</p>"},{"location":"maintenance/#api-key-management","title":"API Key Management","text":"<ul> <li>OpenAI API Key: Monitor usage and ensure the API key remains valid. Rotate keys periodically for security.</li> <li>LangChain API Key: If using LangSmith, monitor usage and ensure the key is valid.</li> </ul>"},{"location":"maintenance/#dependency-updates","title":"Dependency Updates","text":"<p>Regularly update dependencies to: - Fix security vulnerabilities - Get bug fixes and performance improvements - Stay compatible with external services (Overpass API, OpenAI API, etc.)</p>"},{"location":"maintenance/#log-monitoring","title":"Log Monitoring","text":"<p>Monitor application logs for: - Error patterns that may indicate issues - Performance degradation - Authentication/authorization failures - Downstream service failures (OpenAI, Overpass API, etc.)</p>"},{"location":"maintenance/#database-connection-management","title":"Database Connection Management","text":"<p>For the <code>/text2sql</code> endpoint: - Monitor database connection performance - Ensure proper connection pooling if implemented - Validate that user-provided database credentials are handled securely</p>"},{"location":"maintenance/#troubleshooting","title":"Troubleshooting","text":""},{"location":"maintenance/#common-issues","title":"Common Issues","text":"<ol> <li>OpenAI API Errors: Check API key validity, rate limits, and service status.</li> <li>Overpass API Timeouts: The Overpass API may be slow or unavailable. Consider implementing retries or fallback mechanisms.</li> <li>Database Connection Failures: Verify network connectivity and database credentials provided by users.</li> <li>Authentication Failures: Verify OIDC provider configuration and token validity.</li> </ol>"},{"location":"maintenance/#performance-optimization","title":"Performance Optimization","text":"<ul> <li>Monitor response times for each endpoint</li> <li>Consider caching frequently accessed data (e.g., Wikidata lookups)</li> <li>Optimize LLM prompts to reduce token usage and improve response times</li> <li>Monitor and optimize database query performance for text-to-SQL operations</li> </ul>"},{"location":"onboarding/","title":"Developer Onboarding Guide","text":"<p>Welcome to the developer guide for the In-Dataset Discovery Service.</p> <p>This document provides a comprehensive overview of the service's architecture and a step-by-step guide for extending it with new features, agents, and tools.</p>"},{"location":"onboarding/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Service Overview</li> <li>Core Principles</li> <li>Directory Structure</li> <li>Key Components</li> <li>System Architecture</li> <li>API Layer</li> <li>Agent Layer</li> <li>Tool Layer</li> <li>Adding New Features</li> <li>Adding a New API Endpoint</li> <li>Adding a New Agent</li> <li>Adding a New Tool</li> <li>Development Workflow</li> <li>Local Setup</li> <li>Testing</li> <li>Code Style</li> </ol>"},{"location":"onboarding/#1-service-overview","title":"1. Service Overview","text":"<p>The In-Dataset Discovery Service is a FastAPI-based application that provides natural language interfaces for exploring datasets. It uses LLM capabilities to convert user questions into executable queries and agents to orchestrate complex multi-step tasks.</p>"},{"location":"onboarding/#core-principles","title":"Core Principles","text":"<p>The service is built on a few key principles:</p> <ul> <li>Modularity: Each component (agents, tools, models) is self-contained and can be extended independently.</li> <li>LLM-Powered: Leverages LLM and LangChain for intelligent query generation and processing.</li> <li>Agent-Based: Uses LangGraph to orchestrate complex workflows that combine multiple tools.</li> <li>Type Safety: Uses Pydantic models for request/response validation and type safety.</li> </ul>"},{"location":"onboarding/#directory-structure","title":"Directory Structure","text":"<pre><code>src/\n\u251c\u2500\u2500 app/                    # FastAPI application\n\u2502   \u251c\u2500\u2500 main.py            # API endpoints and routing\n\u2502   \u251c\u2500\u2500 config.py          # Configuration management\n\u2502   \u251c\u2500\u2500 security.py        # Authentication and authorization\n\u2502   \u251c\u2500\u2500 exceptions.py      # Custom exception classes\n\u2502   \u2514\u2500\u2500 logging_config.py  # Logging setup\n\u251c\u2500\u2500 agents/                 # Agent implementations\n\u2502   \u251c\u2500\u2500 geospatial.py      # Geospatial query agent\n\u2502   \u251c\u2500\u2500 text2sql.py        # Text-to-SQL agent\n\u2502   \u251c\u2500\u2500 graph.py           # LangGraph workflow definitions\n\u2502   \u2514\u2500\u2500 tools/             # Agent tools\n\u2502       \u251c\u2500\u2500 geospatial_tools.py\n\u2502       \u2514\u2500\u2500 text2sql_tool.py\n\u2514\u2500\u2500 models/                 # Pydantic models\n    \u2514\u2500\u2500 geospatial_models.py\n</code></pre>"},{"location":"onboarding/#key-components","title":"Key Components","text":"<ol> <li>FastAPI Application (<code>src/app/main.py</code>): Defines all API endpoints and request/response models.</li> <li>Agents (<code>src/agents/</code>): Implement the core logic for processing queries:</li> <li><code>geospatial.py</code>: Handles geospatial queries using Wikidata and Overpass API</li> <li><code>text2sql.py</code>: Converts natural language to SQL queries</li> <li>Tools (<code>src/agents/tools/</code>): Reusable functions that agents can call:</li> <li><code>geospatial_tools.py</code>: Tools for geospatial operations</li> <li><code>text2sql_tool.py</code>: Tools for SQL generation and execution</li> <li>Models (<code>src/agents/models/</code>): Pydantic models for data validation</li> </ol>"},{"location":"onboarding/#2-system-architecture","title":"2. System Architecture","text":""},{"location":"onboarding/#api-layer","title":"API Layer","text":"<p>The API layer (<code>src/app/main.py</code>) handles HTTP requests and responses. Each endpoint: - Validates input using Pydantic models - Handles authentication (if required) - Calls the appropriate agent or tool - Returns structured responses</p>"},{"location":"onboarding/#agent-layer","title":"Agent Layer","text":"<p>Agents are the core processing units that handle specific types of queries:</p> <ul> <li>Geospatial Agent: Processes location-based queries</li> <li>Text-to-SQL Agent: Converts questions to SQL queries</li> </ul>"},{"location":"onboarding/#tool-layer","title":"Tool Layer","text":"<p>Tools are reusable functions that agents can invoke. They encapsulate specific operations like: - Querying external APIs (Overpass, Wikidata) - Generating SQL queries - Executing database queries</p>"},{"location":"onboarding/#3-adding-new-features","title":"3. Adding New Features","text":""},{"location":"onboarding/#adding-a-new-api-endpoint","title":"Adding a New API Endpoint","text":"<ol> <li>Define Request/Response Models in <code>src/app/main.py</code>:</li> </ol> <pre><code>class MyQueryRequest(BaseModel):\n    question: str = Field(..., description=\"The question to process\")\n\nclass MyQueryResponse(BaseModel):\n    answer: str\n    metadata: Dict[str, Any]\n</code></pre> <ol> <li>Create the Endpoint:</li> </ol> <pre><code>@app.post(\"/my-endpoint\", response_model=MyQueryResponse)\nasync def my_endpoint(query: MyQueryRequest):\n    \"\"\"Process a query and return results.\"\"\"\n    # Your logic here\n    result = process_query(query.question)\n    return MyQueryResponse(answer=result, metadata={})\n</code></pre> <ol> <li>Add Error Handling:</li> </ol> <pre><code>try:\n    result = process_query(query.question)\n    return MyQueryResponse(answer=result, metadata={})\nexcept Exception as e:\n    logger.error(\"query_failed\", error=str(e))\n    raise HTTPException(status_code=500, detail=str(e))\n</code></pre>"},{"location":"onboarding/#adding-a-new-agent","title":"Adding a New Agent","text":"<ol> <li>Create Agent File in <code>src/agents/</code>:</li> </ol> <pre><code># src/agents/my_agent.py\nimport structlog\nfrom typing import Dict, Any\n\nlogger = structlog.get_logger(__name__)\n\ndef my_agent(question: str, log: structlog.BoundLogger) -&gt; Dict[str, Any]:\n    \"\"\"Process a question using the my_agent.\"\"\"\n    log.info(\"my_agent_started\", question=question)\n\n    # Your agent logic here\n    result = process_question(question)\n\n    log.info(\"my_agent_completed\", result=result)\n    return result\n</code></pre> <ol> <li>Add Tools (if needed) in <code>src/agents/tools/</code>:</li> </ol> <pre><code># src/agents/tools/my_tool.py\ndef my_tool(param: str) -&gt; Dict[str, Any]:\n    \"\"\"A tool that does something useful.\"\"\"\n    # Tool implementation\n    return {\"result\": \"value\"}\n</code></pre> <ol> <li>Integrate with API in <code>src/app/main.py</code>:</li> </ol> <pre><code>from src.agents.my_agent import my_agent\n\n@app.get(\"/my-endpoint\")\nasync def my_endpoint(question: str, log: structlog.BoundLogger = Depends(get_logger)):\n    return my_agent(question, log)\n</code></pre>"},{"location":"onboarding/#adding-a-new-tool","title":"Adding a New Tool","text":"<ol> <li>Create Tool File in <code>src/agents/tools/</code>:</li> </ol> <pre><code># src/agents/tools/my_tool.py\nfrom typing import Dict, Any\n\ndef my_tool(param1: str, param2: int) -&gt; Dict[str, Any]:\n    \"\"\"\n    Tool description.\n\n    Args:\n        param1: Description of param1\n        param2: Description of param2\n\n    Returns:\n        Dictionary with results\n    \"\"\"\n    # Tool implementation\n    return {\"result\": \"value\"}\n</code></pre> <ol> <li>Use in Agent:</li> </ol> <pre><code>from src.agents.tools.my_tool import my_tool\n\ndef my_agent(question: str) -&gt; Dict[str, Any]:\n    result = my_tool(param1=\"value\", param2=42)\n    return result\n</code></pre>"},{"location":"onboarding/#4-development-workflow","title":"4. Development Workflow","text":""},{"location":"onboarding/#local-setup","title":"Local Setup","text":"<ol> <li> <p>Clone the repository: <pre><code>git clone git@github.com:datagems-eosc/in-data-exploration.git\ncd in-data-exploration\n</code></pre></p> </li> <li> <p>Install dependencies: <pre><code># Using uv (recommended)\nuv sync\n\n# Or using pip\npip install -e .\n</code></pre></p> </li> <li> <p>Set up environment variables: <pre><code>cp .env.template .env\n# Edit .env with your configuration\n</code></pre></p> </li> <li> <p>Run the service: <pre><code># Using uvicorn directly\nuvicorn src.app.main:app --reload --port 8080\n\n# Or using Docker\ndocker build -t in-data-exploration .\ndocker run --env-file .env -p 8080:8080 in-data-exploration\n</code></pre></p> </li> </ol>"},{"location":"onboarding/#testing","title":"Testing","text":"<ol> <li> <p>Run health check: <pre><code>curl http://localhost:8080/health\n</code></pre></p> </li> <li> <p>Test an endpoint: <pre><code>curl \"http://localhost:8080/geospatial?question=What are the coordinates in Berlin?\"\n</code></pre></p> </li> <li> <p>View API documentation:</p> </li> <li>Swagger UI: http://localhost:8080/swagger</li> <li>ReDoc: http://localhost:8080/redoc</li> </ol>"},{"location":"onboarding/#code-style","title":"Code Style","text":"<ul> <li>Follow PEP 8 style guidelines</li> <li>Use type hints for all function parameters and return values</li> <li>Use Pydantic models for data validation</li> <li>Add docstrings to all public functions and classes</li> <li>Use structured logging with <code>structlog</code></li> <li>Handle errors gracefully and return appropriate HTTP status codes</li> </ul>"},{"location":"onboarding/#best-practices","title":"Best Practices","text":"<ol> <li>Error Handling: Always wrap agent calls in try-except blocks and log errors appropriately.</li> <li>Logging: Use structured logging with context (user ID, correlation ID, etc.).</li> <li>Validation: Use Pydantic models for all request/response validation.</li> <li>Security: Follow the security patterns established in <code>src/app/security.py</code> for protected endpoints.</li> <li>Testing: Write tests for new features and ensure they pass before submitting PRs.</li> </ol>"},{"location":"onboarding/#next-steps","title":"Next Steps","text":"<ul> <li>Review the Architecture documentation for more details on system design</li> <li>Check the API Overview to understand existing endpoints</li> <li>Read the Configuration guide for environment setup</li> <li>Explore the source code in <code>src/agents/</code> to see examples of agent implementations</li> </ul>"},{"location":"openapi/","title":"OpenAPI Specification","text":""},{"location":"openapi/#datagems-in-dataset-discovery-service-030","title":"DataGEMS In-Dataset Discovery Service 0.3.0","text":"<p>The In-Dataset Discovery Service provides a secure API for performing natural language exploration queries on structured or unstructured data within datasets. It supports geospatial queries and text-to-SQL conversion using LLM capabilities.</p> Terms of service: https://datagems.eu/terms Contact: DataGEMS helpdesk  License: EUPL-1.2 license"},{"location":"openapi/#servers","title":"Servers","text":"Description URL Development server https://datagems-dev.scayle.es/in-dataset-discovery"},{"location":"openapi/#monitoring","title":"Monitoring","text":""},{"location":"openapi/#get","title":"GET /","text":"<p>Root</p> Description <p>Returns the API status.</p> <p> Response 200 OK </p> application/json <p><pre><code>{\n    \"message\": \"In Data Exploration API is running.\",\n    \"status\": \"success\"\n}\n</code></pre> \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information.</p> Schema of the response body <pre><code>{\n    \"properties\": {\n        \"message\": {\n            \"example\": \"In Data Exploration API is running.\",\n            \"type\": \"string\"\n        },\n        \"status\": {\n            \"example\": \"success\",\n            \"type\": \"string\"\n        }\n    },\n    \"type\": \"object\"\n}\n</code></pre>"},{"location":"openapi/#get-health","title":"GET /health","text":"<p>Health Check</p> Description <p>Checks the availability of the API service. Returns a 200 OK if the service is healthy. This endpoint does not require authentication.</p> <p> Response 200 OK </p> application/json <p><pre><code>{\n    \"status\": \"ok\"\n}\n</code></pre> \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information.</p> Schema of the response body <pre><code>{\n    \"properties\": {\n        \"status\": {\n            \"example\": \"ok\",\n            \"type\": \"string\"\n        }\n    },\n    \"type\": \"object\"\n}\n</code></pre>"},{"location":"openapi/#geospatial","title":"Geospatial","text":""},{"location":"openapi/#get-text2geo","title":"GET /text2geo","text":"<p>Geospatial Query</p> Description <p>Processes a natural language geospatial question, identifies the place using Wikidata, generates an OverpassQL query, and returns geospatial results including coordinates, GeoJSON data, and bounding boxes.</p> <p>Input parameters</p> Parameter In Type Default Nullable Description <code>OAuth2Bearer</code> header string N/A No JWT token for authentication, obtained from the OIDC provider. <code>question</code> query string No The geospatial query question to be processed. <p> Response 200 OK </p> application/json <p><pre><code>{\n    \"most_relevant_wikidata\": {\n        \"place\": \"string\",\n        \"reasoning\": \"string\",\n        \"wiki_id\": \"string\",\n        \"wiki_properties\": {\n            \"aliases\": \"string\",\n            \"coordinate location\": \"string\",\n            \"country\": \"string\",\n            \"description\": \"string\",\n            \"found_osm_json\": true,\n            \"instance of\": \"string\",\n            \"label\": \"string\",\n            \"located in the administrative territorial entity\": \"string\",\n            \"part of\": \"string\"\n        }\n    },\n    \"oql\": {\n        \"OQL\": \"string\",\n        \"reasoning\": \"string\"\n    },\n    \"place\": \"string\",\n    \"results\": {\n        \"bounds\": {\n            \"maxlat\": 10.12,\n            \"maxlon\": 10.12,\n            \"minlat\": 10.12,\n            \"minlon\": 10.12\n        },\n        \"center\": [\n            10.12\n        ],\n        \"geojson_data\": {},\n        \"points\": [\n            {\n                \"lat\": \"string\",\n                \"lon\": \"string\"\n            }\n        ]\n    }\n}\n</code></pre> \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information.</p> Schema of the response body <pre><code>{\n    \"properties\": {\n        \"most_relevant_wikidata\": {\n            \"$ref\": \"#/components/schemas/MostRelevantWikidata\",\n            \"description\": \"The most relevant Wikidata entity related to the query.\"\n        },\n        \"oql\": {\n            \"$ref\": \"#/components/schemas/OQLResponse\",\n            \"description\": \"The OverpassQL query generated from the geospatial question.\"\n        },\n        \"place\": {\n            \"description\": \"The place identified in the geospatial query.\",\n            \"type\": \"string\"\n        },\n        \"results\": {\n            \"$ref\": \"#/components/schemas/GeospatialResults\",\n            \"description\": \"The results of the OverpassQL query includes points, bounding box, multipolygons and centroid.\"\n        }\n    },\n    \"required\": [\n        \"place\",\n        \"most_relevant_wikidata\",\n        \"oql\",\n        \"results\"\n    ],\n    \"type\": \"object\"\n}\n</code></pre> <p> Response 422 Unprocessable Content </p> application/json <pre><code>{\n    \"code\": 102,\n    \"error\": \"validation error\",\n    \"message\": [\n        {\n            \"Key\": \"question\",\n            \"Value\": [\n                \"Field required\"\n            ]\n        }\n    ]\n}\n</code></pre> Schema of the response body <pre><code>{\n    \"properties\": {\n        \"code\": {\n            \"example\": 102,\n            \"type\": \"integer\"\n        },\n        \"error\": {\n            \"example\": \"validation error\",\n            \"type\": \"string\"\n        },\n        \"message\": {\n            \"items\": {\n                \"$ref\": \"#/components/schemas/ValidationErrorDetail\"\n            },\n            \"type\": \"array\"\n        }\n    },\n    \"required\": [\n        \"code\",\n        \"error\",\n        \"message\"\n    ],\n    \"type\": \"object\"\n}\n</code></pre> <p> Response 500 Internal Server Error </p> application/json <p><pre><code>{\n    \"code\": 0,\n    \"error\": \"string\"\n}\n</code></pre> \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information.</p> Schema of the response body <pre><code>{\n    \"properties\": {\n        \"code\": {\n            \"description\": \"HTTP status code\",\n            \"type\": \"integer\"\n        },\n        \"error\": {\n            \"description\": \"Error message\",\n            \"type\": \"string\"\n        }\n    },\n    \"required\": [\n        \"code\",\n        \"error\"\n    ],\n    \"type\": \"object\"\n}\n</code></pre>"},{"location":"openapi/#text-to-sql","title":"Text-to-SQL","text":""},{"location":"openapi/#post-text2sql","title":"POST /text2sql","text":"<p>Text-to-SQL Query</p> Description <p>Takes a natural language question and database connection information, generates a SQL query using LLM capabilities, and executes it to return results.</p> <p>Input parameters</p> Parameter In Type Default Nullable Description <code>OAuth2Bearer</code> header string N/A No JWT token for authentication, obtained from the OIDC provider. <p>Request body</p> application/json <p><pre><code>{\n    \"parameters\": {\n        \"db_info\": {\n            \"db_database\": \"string\",\n            \"db_host\": \"string\",\n            \"db_pass\": \"string\",\n            \"db_port\": 0,\n            \"db_schema\": \"string\",\n            \"db_username\": \"string\"\n        },\n        \"results\": {\n            \"points\": [\n                [\n                    10.12\n                ]\n            ]\n        }\n    },\n    \"question\": \"What are the average mean temperatures in the coordinates (lon, lat) in year 2020?\"\n}\n</code></pre> \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information.</p> Schema of the request body <pre><code>{\n    \"properties\": {\n        \"parameters\": {\n            \"$ref\": \"#/components/schemas/SQLQueryParameters\",\n            \"description\": \"The parameters for the SQL query, including results.\"\n        },\n        \"question\": {\n            \"description\": \"The text question to be converted to SQL.\",\n            \"example\": \"What are the average mean temperatures in the coordinates (lon, lat) in year 2020?\",\n            \"type\": \"string\"\n        }\n    },\n    \"required\": [\n        \"question\",\n        \"parameters\"\n    ],\n    \"type\": \"object\"\n}\n</code></pre> <p> Response 200 OK </p> application/json <p><pre><code>{\n    \"input_params\": [\n        {\n            \"lat\": 10.12,\n            \"lon\": 10.12\n        }\n    ],\n    \"message\": \"string\",\n    \"model_name\": \"string\",\n    \"output_params\": {\n        \"coordinates\": [\n            \"string\"\n        ]\n    },\n    \"params\": {},\n    \"question\": \"string\",\n    \"reasoning\": \"string\",\n    \"sql_pattern\": \"string\",\n    \"sql_query\": \"string\",\n    \"sql_results\": {\n        \"data\": [\n            {}\n        ],\n        \"status\": \"string\"\n    },\n    \"status\": \"string\"\n}\n</code></pre> \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information.</p> Schema of the response body <pre><code>{\n    \"properties\": {\n        \"input_params\": {\n            \"description\": \"Input parameters with coordinates.\",\n            \"items\": {\n                \"$ref\": \"#/components/schemas/InputParam\"\n            },\n            \"type\": \"array\"\n        },\n        \"message\": {\n            \"description\": \"Message describing the operation result.\",\n            \"type\": \"string\"\n        },\n        \"model_name\": {\n            \"description\": \"The model name used for SQL generation.\",\n            \"type\": \"string\"\n        },\n        \"output_params\": {\n            \"$ref\": \"#/components/schemas/OutputParams\",\n            \"description\": \"Output parameter definitions.\"\n        },\n        \"params\": {\n            \"additionalProperties\": true,\n            \"description\": \"Parameters used in the query.\",\n            \"type\": \"object\"\n        },\n        \"question\": {\n            \"description\": \"The original question that was converted to SQL.\",\n            \"type\": \"string\"\n        },\n        \"reasoning\": {\n            \"description\": \"Reasoning behind the SQL query generation.\",\n            \"type\": \"string\"\n        },\n        \"sql_pattern\": {\n            \"description\": \"The SQL pattern/template generated.\",\n            \"type\": \"string\"\n        },\n        \"sql_query\": {\n            \"description\": \"The final SQL query with parameters filled in.\",\n            \"type\": \"string\"\n        },\n        \"sql_results\": {\n            \"$ref\": \"#/components/schemas/SQLResults\",\n            \"description\": \"Results from executing the SQL query.\"\n        },\n        \"status\": {\n            \"description\": \"Status of the operation.\",\n            \"type\": \"string\"\n        }\n    },\n    \"required\": [\n        \"status\",\n        \"message\",\n        \"params\",\n        \"question\",\n        \"model_name\",\n        \"sql_pattern\",\n        \"input_params\",\n        \"output_params\",\n        \"reasoning\",\n        \"sql_query\",\n        \"sql_results\"\n    ],\n    \"type\": \"object\"\n}\n</code></pre> <p> Response 422 Unprocessable Content </p> application/json <pre><code>{\n    \"code\": 102,\n    \"error\": \"validation error\",\n    \"message\": [\n        {\n            \"Key\": \"question\",\n            \"Value\": [\n                \"Field required\"\n            ]\n        },\n        {\n            \"Key\": \"parameters.results.points\",\n            \"Value\": [\n                \"Field required\"\n            ]\n        }\n    ]\n}\n</code></pre> Schema of the response body <pre><code>{\n    \"properties\": {\n        \"code\": {\n            \"example\": 102,\n            \"type\": \"integer\"\n        },\n        \"error\": {\n            \"example\": \"validation error\",\n            \"type\": \"string\"\n        },\n        \"message\": {\n            \"items\": {\n                \"$ref\": \"#/components/schemas/ValidationErrorDetail\"\n            },\n            \"type\": \"array\"\n        }\n    },\n    \"required\": [\n        \"code\",\n        \"error\",\n        \"message\"\n    ],\n    \"type\": \"object\"\n}\n</code></pre> <p> Response 500 Internal Server Error </p> application/json <p><pre><code>{\n    \"code\": 0,\n    \"error\": \"string\"\n}\n</code></pre> \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information.</p> Schema of the response body <pre><code>{\n    \"properties\": {\n        \"code\": {\n            \"description\": \"HTTP status code\",\n            \"type\": \"integer\"\n        },\n        \"error\": {\n            \"description\": \"Error message\",\n            \"type\": \"string\"\n        }\n    },\n    \"required\": [\n        \"code\",\n        \"error\"\n    ],\n    \"type\": \"object\"\n}\n</code></pre>"},{"location":"openapi/#schemas","title":"Schemas","text":""},{"location":"openapi/#bounds","title":"Bounds","text":"Name Type <code>maxlat</code> number(float) <code>maxlon</code> number(float) <code>minlat</code> number(float) <code>minlon</code> number(float)"},{"location":"openapi/#errorresponse","title":"ErrorResponse","text":"Name Type <code>code</code> integer <code>error</code> string"},{"location":"openapi/#faileddependencymessage","title":"FailedDependencyMessage","text":"Name Type <code>correlationId</code> string| null <code>payload</code> <code>source</code> string <code>statusCode</code> integer"},{"location":"openapi/#faileddependencyresponse","title":"FailedDependencyResponse","text":"Name Type <code>code</code> integer <code>error</code> string <code>message</code> FailedDependencyMessage"},{"location":"openapi/#geospatialresponse","title":"GeospatialResponse","text":"Name Type <code>most_relevant_wikidata</code> MostRelevantWikidata <code>oql</code> OQLResponse <code>place</code> string <code>results</code> GeospatialResults"},{"location":"openapi/#geospatialresults","title":"GeospatialResults","text":"Name Type <code>bounds</code> Bounds <code>center</code> Array&lt;number(float)&gt; <code>geojson_data</code> <code>points</code> Array&lt;Point&gt;"},{"location":"openapi/#inputparam","title":"InputParam","text":"Name Type <code>lat</code> number(float) <code>lon</code> number(float)"},{"location":"openapi/#mostrelevantwikidata","title":"MostRelevantWikidata","text":"Name Type <code>place</code> string <code>reasoning</code> string <code>wiki_id</code> string <code>wiki_properties</code> WikiProperties"},{"location":"openapi/#oqlresponse","title":"OQLResponse","text":"Name Type <code>OQL</code> string <code>reasoning</code> string"},{"location":"openapi/#outputparams","title":"OutputParams","text":"Name Type <code>coordinates</code> Array&lt;string&gt;"},{"location":"openapi/#point","title":"Point","text":"Name Type <code>lat</code> string <code>lon</code> string"},{"location":"openapi/#resultsmodel","title":"ResultsModel","text":"Name Type <code>points</code> Array&lt;Array&lt;number(float)&gt;&gt;"},{"location":"openapi/#sqlqueryparameters","title":"SQLQueryParameters","text":"Name Type <code>db_info</code> Properties: <code>db_database, db_host, db_pass, db_port, db_schema, db_username</code> <code>results</code> ResultsModel"},{"location":"openapi/#sqlresults","title":"SQLResults","text":"Name Type <code>data</code> Array&lt;&gt; <code>status</code> string"},{"location":"openapi/#text2sqlquery","title":"Text2SQLQuery","text":"Name Type <code>parameters</code> SQLQueryParameters <code>question</code> string"},{"location":"openapi/#text2sqlresponse","title":"Text2SQLResponse","text":"Name Type <code>input_params</code> Array&lt;InputParam&gt; <code>message</code> string <code>model_name</code> string <code>output_params</code> OutputParams <code>params</code> <code>question</code> string <code>reasoning</code> string <code>sql_pattern</code> string <code>sql_query</code> string <code>sql_results</code> SQLResults <code>status</code> string"},{"location":"openapi/#validationerrordetail","title":"ValidationErrorDetail","text":"Name Type <code>Key</code> string <code>Value</code> Array&lt;string&gt;"},{"location":"openapi/#validationerrorresponse","title":"ValidationErrorResponse","text":"Name Type <code>code</code> integer <code>error</code> string <code>message</code> Array&lt;ValidationErrorDetail&gt;"},{"location":"openapi/#wikiproperties","title":"WikiProperties","text":"Name Type <code>aliases</code> string <code>coordinate location</code> string <code>country</code> string <code>description</code> string <code>found_osm_json</code> boolean <code>instance of</code> string <code>label</code> string <code>located in the administrative territorial entity</code> string <code>part of</code> string"},{"location":"openapi/#security-schemes","title":"Security schemes","text":"Name Type Scheme Description OAuth2Bearer http bearer JWT token for authentication, obtained from the OIDC provider."},{"location":"openapi/#tags","title":"Tags","text":"Name Description Text-to-Geo Endpoints for performing geospatial queries using natural language. Text-to-SQL Endpoints for converting natural language questions to SQL queries. Monitoring Endpoints for monitoring the service health."},{"location":"qa/","title":"Quality Assurance","text":"<p>Quality assurance (QA) combines automated static analysis, vulnerability scanning, and API-level integration testing to ensure the service functions as expected.</p>"},{"location":"qa/#code-analysis","title":"Code Analysis","text":"<p>We use GitHub CodeQL for static analysis of the source code. This process helps identify potential bugs, security vulnerabilities, and quality issues before they impact production.</p> <ul> <li>Technology: GitHub CodeQL</li> <li>Targets: The analysis runs on both the Python source code and the GitHub Actions workflow files themselves.</li> <li>Checks: It uses a set of queries to find common vulnerabilities (e.g., injection flaws, insecure data handling), bugs, and code quality anti-patterns.</li> <li>Execution: The analysis is performed by the on-demand <code>code-scan-on-demand.yml</code> workflow. Results and alerts are available directly in the repository's \"Security\" tab.</li> </ul>"},{"location":"qa/#code-metrics","title":"Code Metrics","text":"<p>To ensure the long-term maintainability and readability of the code, we use the Radon library to generate code metrics.</p> <ul> <li>Technology: Radon</li> <li>Metrics:<ul> <li>Maintainability Index (MI): A score from 0-100 indicating how easy the code is to maintain (higher is better).</li> <li>Cyclomatic Complexity (CC): Measures the number of independent paths through the code. A lower score indicates simpler, less complex code.</li> <li>Lines of Code (LOC): Provides raw metrics on the size of the codebase.</li> </ul> </li> <li>Execution: The metrics are generated by the on-demand <code>code-metrics-on-demand.yml</code> workflow, which produces a downloadable <code>code-metrics-report.txt</code> artifact.</li> </ul>"},{"location":"qa/#vulnerability-checks","title":"Vulnerability Checks","text":"<p>We use Trivy to scan for known vulnerabilities in our dependencies and container image, ensuring the service is secure from common threats.</p> <ul> <li>Technology: Trivy</li> <li>Scans:<ol> <li>Configuration Scan: Scans the <code>Dockerfile</code> and other repository files for security misconfigurations.</li> <li>Image Scan: Scans the final Docker image for known vulnerabilities (CVEs) with <code>CRITICAL</code> or <code>HIGH</code> severity in its OS packages and Python libraries.</li> </ol> </li> <li>Execution: The scan is performed by the on-demand <code>vulnerability-scan-on-demand.yml</code> workflow. It requires an image tag as input and uploads detailed JSON reports as build artifacts.</li> </ul>"},{"location":"qa/#testing","title":"Testing","text":"<p>The service's functionality is validated through API-level integration tests using Postman and its command-line runner, Newman.</p> <ul> <li> <p>Test Suite: The test cases are defined in the Postman collection located at <code>tests/in-data-exploration-api-tests.postman_collection.json</code> (if available).</p> </li> <li> <p>Test Cases: The suite may include the following tests:</p> <ul> <li>Health Check: Verifies that the <code>/health</code> endpoint is available and returns a <code>200 OK</code> status, indicating the service is healthy.</li> <li>Geospatial Query - Valid Request: Executes a valid geospatial query against the <code>/geospatial</code> endpoint and asserts that the response is successful (<code>200 OK</code>) and has the correct structure.</li> <li>Text-to-SQL Query - Valid Request: Executes a valid text-to-SQL query against the <code>/text2sql</code> endpoint and asserts that the response is successful (<code>200 OK</code>) and contains the generated SQL and results.</li> </ul> </li> </ul>"},{"location":"qa/#how-to-run-tests","title":"How to Run Tests","text":"<p>The API tests are designed to be run automatically via a GitHub Actions workflow.</p> <ul> <li>Workflow File: <code>.github/workflows/test-on-demand.yml</code></li> <li>Trigger: The workflow is triggered manually (<code>workflow_dispatch</code>) from the Actions tab in the GitHub repository.</li> <li>Process:<ol> <li>Navigate to the \"Actions\" tab and select the \"Test Scenario (On-Demand)\" workflow.</li> <li>Click \"Run workflow\". You will be prompted to enter a <code>tag</code> (e.g., <code>main</code> or a specific version like <code>v1.0.0</code>) to test against.</li> <li>The workflow checks out the specified version of the code.</li> <li>It then uses the official <code>postman/newman</code> Docker image to execute the test collection.</li> <li>All necessary environment variables (API base URL, credentials, etc.) are securely injected into the test run from the repository's secrets.</li> </ol> </li> </ul>"},{"location":"qa/#expected-output","title":"Expected Output","text":"<p>A successful test run will produce the following summary table in the GitHub Actions log:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                         \u2502            executed \u2502             failed \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502              iterations \u2502                   1 \u2502                  0 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                requests \u2502                   4 \u2502                  0 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502            test-scripts \u2502                   4 \u2502                  0 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502      prerequest-scripts \u2502                   0 \u2502                  0 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502              assertions \u2502                   4 \u2502                  0 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 total run duration: 1674ms                                         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 total data received: 6.9kB (approx)                                \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 average response time: 396ms [min: 147ms, max: 645ms, s.d.: 205ms] \u2502\n</code></pre>"},{"location":"security/","title":"Security","text":""},{"location":"security/#authentication","title":"Authentication","text":"<p>Authentication is handled via the OAuth 2.0 and OpenID Connect (OIDC) protocols. Every request to a protected endpoint must include a valid JSON Web Token (JWT) in the <code>Authorization</code> header as a Bearer token.</p> <p>The service validates incoming JWTs against the OIDC provider's public keys. The token's signature, issuer, and audience are all verified to ensure its authenticity.</p>"},{"location":"security/#authorization","title":"Authorization","text":"<p>Authorization is role-based. The service checks for specific roles within the validated JWT's claims.</p> <ul> <li>Required Roles: The <code>/geospatial</code> and <code>/text2sql</code> endpoints may have different authentication requirements depending on the deployment configuration. The <code>/health</code> endpoint is publicly accessible.</li> </ul> <p>If a user attempts to access an endpoint without the required role, the API will respond with a <code>403 Forbidden</code> error.</p>"},{"location":"security/#dataset-level-permissions","title":"Dataset-Level Permissions","text":"<p>In addition to role-based access, the service can enforce dataset-level permissions when integrated with the DataGEMS Gateway.</p> <ul> <li>Database Access:<ul> <li>For <code>/text2sql</code> endpoints, database access is controlled through the database connection information provided in the request. Users must have valid credentials for the databases they wish to query.</li> <li>The service does not store or manage database credentials; they must be provided with each request.</li> </ul> </li> </ul>"}]}